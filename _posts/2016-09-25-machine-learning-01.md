---
layout: post
title:  "机器学习 第一周"
date:   2016-09-25 21:33:54
categories: machine-learning
tags: Stanford AndrewNG
excerpt: 最近在补Coursera上NG的机器学习，写一些博文记录我的学习过程。
---

## 前言

一直都听说Coursera上这门机器学习质量很高，整个大三都比较偷懒，一直没空去来学，到了求职季那个后悔啊……当下是信息爆炸的年代，没有一技之长的人，肯定是难以生存的，这里所说的技术，显然是指十分通用的、不会被淘汰的技术，而机器学习在我看来很符合这方面的标准。来百度实习后，运气好亲眼见到了NG，自己感觉也是一个不错的契机，于是痛下决心开始了Coursera上的机器学习之旅，同时在Github上架了个博客，因为对前端可以说一窍不通，所以完全copy了别人的模板……诶，书到用时方恨少啊！

***

## Introduction

什么是机器学习？视频里有一句话说的非常好： There's a science of getting computers to learn without being explicitly programmed. 以往的编程模式，包括各种花哨的算法、数据结构，其本质无非是利用各种条件跳转实现的控制流，优点是对于意料之中的情况绝不会出错，缺点也很显然，做不到举一反三————哪怕是很显然的推论。因此对设计者的要求极高，有时候看上去很隐蔽的一处不合理的地方，连设计者本人也没有想到会造成极其离谱的结果，离谱到什么程度？这里不妨说一个前阵子见到的段子：
> 阿里巴巴收购KFC，起因在于马云对他的秘书说，“帮我买下肯德基”，在他本人得知了这一严重后果时，第一反应是：“快！帮我叫住小王！我刚才让他帮我买的中南海是烟！”

可以看到在这个例子里，马老板的部下扮演了传统计算机的角色，其精准的理解能力和强大的执行力令人咋舌，但是恐怕老板此时心里在盘算着把他们炒了，换“机器学习”来。

### 监督式学习
第一周就是介绍下基本概念啦，课上举了两个监督学习的例子，分别对应了回归问题和分类问题。

* 回归问题

给出一组数据，其中包含了多组房屋面积和面积单价的关系，现在你朋友有栋X平米的房子，请你根据前面的数据预测出他能卖多少钱。

* 分类问题

给出一组数据，其中包含了关于这个肿瘤的数据（size），以及它们是否为良性/恶性肿瘤，现在来了一个新的病人，请你帮他预测他的肿瘤为良性还是恶性。

### 非监督式学习

* 聚类问题

谷歌新闻与别的新闻发布平台不一样，他没有小编，是完全由机器完成的，这其中涉及到对新闻按话题进行分类。

* 鸡尾酒宴问题

这是一个语音识别领域的问题，在多人同时讲话的情况下如何以人为单位提取出独立的声音？

### 测验&心得

这里有一点糟糕，测验是第二天才做的，第一遍只拿了40分，原因在于以上四个东西没能分得很清。所以这里要用最言简意赅的语言重新表述一下：

**监督式学习的特点是数据集对于输入总有一个"right answer"**，例如：房价是多少？ansewer：3万一平。反过来看谷歌新闻的例子，我们的数据集里有的是新闻本身，而不会有任何关于它的分类的信息。

**回归问题是要预测连续值输出，而分类问题是预测离散值输出**，这也很好理解，房价的范围是无穷的，而肿瘤问题的输出只有两个：是或者不是。

***

## 单一变量的线性回归问题

标题一下子变的有那么一点高大上的意味了，实质只是限定了“一个变量”“线性”两个强约束条件来研究最一般的情况。

**代价函数**可以理解为衡量预测值与真实值偏差的指标，这里以**平方误差函数**的形式出现，话不多说直接上图。

![function](http://p1.bpimg.com/4851/f932868bfbb53f4f.jpg)

通俗的说，这说明了这样一个问题，**假设房价只受一个因素的影响（且成正比），那么预测函数是线性函数，代价函数是二维函数。**并且这个二维函数的最低点意味着代价最小————即预测最准确，图中为0代表完全准确，拟合曲线与原数据集完全重合。当然这实在是过于理想化了，但我们从简单的事物出发，看的会更加清晰一点，这里很明显，对于这一类机器学习问题，我们的目标是最小化代价函数。

## 梯度下降

一图流，对代价函数的求导过程有兴趣可以自己推一下，还是很直观的。用导数来计算步长很有好处，这意味着越接近极值步伐就越缓慢，到了极值导数为0就不会再动了。

![function](http://p1.bpimg.com/4851/253f1d729b9b0885.jpg)
