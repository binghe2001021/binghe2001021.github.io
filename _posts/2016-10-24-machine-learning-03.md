## 分类问题与逻辑回归

前面提到分类问题与回归问题的区别在于预测离散值，这里还是从最简单的情况出发，从探讨**二值分类问题**开始（0或1）。我们还是以与线性回归相同的思路去做，只不过这里的建模有些区别：

![](http://i1.piimg.com/4851/18661954d3253b57.png)

假设函数如上图所示，其中g(z)是一个输出值在0-1之间的S型函数：

![](http://i1.piimg.com/4851/e047116c850e8ef4.png)

假设函数h的作用是给出预测值是0或1的概率，而(θ^T)x就比较有意思，在线性回归的例子里，它直接就代表着假设函数，而在逻辑回归问题中，它代表着**决策边界**，课上给出了一个很直观的例子：

![](http://p1.bpimg.com/4851/d5dfe10225f51f08.png)

在这个例子中，h(x) = g(-3+x1+x2), 即θ0=-3，θ1=θ2=1，则该线性函数就是如图所示的直线，它将两部分数据完美的划分开来。θ的取值代表了假设函数的性质，我们的目的跟之前线性回归的情况更加接近：找到这样的θ值，而如何去确定我们想要的θ值，则是通过训练数据得到。

***

## 代价函数

![](http://p1.bqimg.com/4851/b77c027e12479d06.png)

首先为什么取log函数的问题，因为预测值越离谱，就要用越大的力度去惩罚目前的学习算法（以肿瘤问题讲，假设病人没有肿瘤，你的算法泡出来是百分百有肿瘤，那你可能会被人砍了-_-||，但是哪怕泡出来是99.9%，那也算是有辩解的余地），举出这个情况的例子：

![](http://p1.bqimg.com/4851/7b5cab9e9df8b050.png)

这个图像的意思是x轴代表h的值，值为0时说明预测肿瘤概率为0，预测正确，代价为0，为1的时候后果就不堪设想了……

下面是**代价函数**，将y代入假设函数可以避免分情况讨论。并且这样一来偏导数也就可以计算出来了，

![](http://i1.piimg.com/567571/0f152d66a35720ca.png)

![](http://p1.bpimg.com/567571/221645e4f5cdc24a.png)

发现了什么？没错，这个用作梯度下降的偏导数，跟线性回归是一样的！这是巧合吗？是否意味着逻辑回归和线性回归本质上是相同的？NG在视频里说的很清楚，还是不同的。

> 逻辑**回归**这个名字的由来是历史原因，它跟回归算法没有关系。

> 在线性回归中，h=(θ^T)X，代表着预测函数，而(θ^T)X在逻辑回归中代表的是决策边界，是不同的概念。

***

--未完待续--

